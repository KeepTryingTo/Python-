{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-17T07:23:37.913707Z","iopub.execute_input":"2023-05-17T07:23:37.914466Z","iopub.status.idle":"2023-05-17T07:23:37.919956Z","shell.execute_reply.started":"2023-05-17T07:23:37.914429Z","shell.execute_reply":"2023-05-17T07:23:37.918889Z"},"editable":false,"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n@Author : Keep_Trying_Go\n@Major  : Computer Science and Technology\n@Hobby  : Computer Vision\n@Time   : 2023/5/16 15:23\n\"\"\"\n\nimport os\nimport torch\nimport albumentations\nfrom torchvision import transforms\nfrom albumentations.pytorch import ToTensorV2\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nDATASET_DIR = \"data/img_align_celeba\"\nTRAIN_DIR = \"data/celeba/train\"\nVAL_DIR = \"data/celeba/val\"\nWIDTH,HEIGHT = 128,128\nLEARNING_RATIO = 1e-4\nBATCH_SIZE_T = 16\nBATCH_SIZE_V = 16\nSHUFFLE = True\nNUM_WORKERS = 0\nSAVE_MODELS = \"models\"\nLOAD_MODELS = \"models\"\nSAVE_IMAGES = \"images\"\nNUM_EPOCHS = 1\nDOWNSCALE = 4\nNEW_SIZE_W,NEW_SIZE_H = WIDTH // DOWNSCALE,HEIGHT // DOWNSCALE\nLOSS_RATIO = 1e-3\nBEAT1 = 0.9\nBEAT2 = 0.9\nEPSILON = 1e-8\n\n#对32 x 32和128 x 128的图像进行增强\nTransform = albumentations.Compose([\n    albumentations.HorizontalFlip(p = 0.5),\n    albumentations.VerticalFlip(p = 0.5),\n    albumentations.RandomBrightness(limit=0.2),\n    albumentations.HueSaturationValue(hue_shift_limit=20,sat_shift_limit=30),\n    albumentations.RandomContrast(limit=0.2),\n    albumentations.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5]),\n    ToTensorV2()\n])\n#对128 x 128的图像进行裁剪到32 x 32\ndown_Transform = albumentations.Compose([\n    albumentations.Resize(height=NEW_SIZE_W,width=NEW_SIZE_H)\n])\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n@Author : Keep_Trying_Go\n@Major  : Computer Science and Technology\n@Hobby  : Computer Vision\n@Time   : 2023/5/16 15:53\n\"\"\"\n\nimport os\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import DataLoader,Dataset\n\nclass MyDataset(Dataset):\n    def __init__(self,root_dir):\n        super(MyDataset, self).__init__()\n        self.root_dir = root_dir\n        self.imgs_list = os.listdir(root_dir)\n\n    def __len__(self):\n        return len(self.imgs_list)\n\n    def __getitem__(self, index):\n        img_name = self.imgs_list[index]\n        img_path = os.path.join(self.root_dir,img_name)\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        labels = Transform(image=image)[\"image\"]\n        features = Transform(image=down_Transform(image=image)[\"image\"])[\"image\"]\n        return labels,features\n\nif __name__ == '__main__':\n    mydataT = MyDataset(\"/kaggle/input/celebadataset/celeba/train\")\n    print(mydataT.__len__())\n    print(\"labels.shape: {}--feautres.shape: {}\".format(np.shape(mydataT[0][0]),np.shape(mydataT[0][1])))\n    mydataV = MyDataset(\"/kaggle/input/celebadataset/celeba/val\")\n    print(mydataV.__len__())\n    print(\"labels.shape: {}--feautres.shape: {}\".format(np.shape(mydataV[0][0]), np.shape(mydataV[0][1])))","metadata":{"execution":{"iopub.status.busy":"2023-05-17T07:23:39.826320Z","iopub.execute_input":"2023-05-17T07:23:39.826629Z","iopub.status.idle":"2023-05-17T07:23:40.132510Z","shell.execute_reply.started":"2023-05-17T07:23:39.826602Z","shell.execute_reply":"2023-05-17T07:23:40.131461Z"},"editable":false,"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"10319\nlabels.shape: torch.Size([3, 128, 128])--feautres.shape: torch.Size([3, 32, 32])\n1147\nlabels.shape: torch.Size([3, 128, 128])--feautres.shape: torch.Size([3, 32, 32])\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\n@Author : Keep_Trying_Go\n@Major  : Computer Science and Technology\n@Hobby  : Computer Vision\n@Time   : 2023/5/16 14:30\n\"\"\"\n\nimport torch\nfrom torchinfo import summary\n\nclass ConvBlock(torch.nn.Module):\n    def __init__(self,in_channels,out_channels):\n        super(ConvBlock, self).__init__()\n        self.conv1 = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels = in_channels,out_channels=out_channels,kernel_size=(3,3),\n                            stride=(1,1),padding=(1,1),bias=False),\n#             torch.nn.BatchNorm2d(num_features=out_channels),\n            torch.nn.LeakyReLU(negative_slope=0.01)\n        )\n        self.conv2 = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=(3, 3),\n                            stride=(2, 2), padding=(1, 1), bias=False),\n#             torch.nn.BatchNorm2d(num_features=out_channels),\n            torch.nn.LeakyReLU(negative_slope=0.01)\n        )\n    def forward(self,x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return x\n\nclass Discriminator(torch.nn.Module):\n    def __init__(self,in_channels,out_channels,data_format = \"channels_first\"):\n        super(Discriminator, self).__init__()\n        if data_format == \"channels_first\":\n            self._input_shape = [-1,3,128,128]\n            self.bn_axis = 1\n        else:\n            assert data_format == \"channels_last\"\n            self._input_shape=[-1,128,128,3]\n            self.bn_axis = 3\n\n        self.conv1 = ConvBlock(in_channels=in_channels,out_channels = 64)\n        self.conv2 = ConvBlock(in_channels=64,out_channels=128)\n        self.conv3 = ConvBlock(in_channels=128,out_channels=256)\n        self.conv4 = ConvBlock(in_channels=256,out_channels=512)\n        # self.fc1 = torch.nn.Linear(in_features=512 * 8 * 8,out_features=256)\n        # self.fc2 = torch.nn.Linear(in_features=256,out_features=out_channels)\n        self.conv5 = torch.nn.Conv2d(in_channels=512, out_channels=out_channels, kernel_size=(3, 3),\n                            stride=(2, 2), padding=(1, 1), bias=False)\n        self.sigmoid = torch.nn.Sigmoid()\n\n    def forward(self,x):\n        # x = torch.reshape(x,self._input_shape)\n\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n\n        # x = x.view(-1,512 * 8 * 8)\n        # x = self.fc1(x)\n        # x = self.fc2(x)\n        x = self.conv5(x)\n        x = self.sigmoid(x)\n\n        return x\n\nif __name__ == '__main__':\n    x = torch.randn(size = (1,3,128,128))\n    model = Discriminator(in_channels=3,out_channels=1)\n    summary(model,input_size=(1,3,128,128))\n    print(model(x).shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T07:38:23.914208Z","iopub.execute_input":"2023-05-17T07:38:23.915239Z","iopub.status.idle":"2023-05-17T07:38:24.081377Z","shell.execute_reply.started":"2023-05-17T07:38:23.915197Z","shell.execute_reply":"2023-05-17T07:38:24.080180Z"},"editable":false,"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"torch.Size([1, 1, 4, 4])\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\n@Author : Keep_Trying_Go\n@Major  : Computer Science and Technology\n@Hobby  : Computer Vision\n@Time   : 2023/5/16 13:27\n\"\"\"\n\nimport torch\nimport numpy as np\nfrom torchinfo import summary\n\n#残差模块\nclass _IdentityBlock(torch.nn.Module):\n    #data_format判断输入图像的通道C位置[w,h,c]或者[c,w,h]\n    def __init__(self,in_channels,out_channels,stride = (1,1),data_format = \"channels_first\"):\n        super(_IdentityBlock, self).__init__()\n        self.bn_axis = 1 if data_format == \"channels_first\" else 3\n        self.conv = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=(3,3),\n                            stride=stride,padding=(1,1),bias=False),\n#             torch.nn.BatchNorm2d(num_features=out_channels),\n            torch.nn.PReLU(num_parameters=1),\n            torch.nn.Conv2d(in_channels = out_channels,out_channels = out_channels,kernel_size=(3,3),\n                            stride=(1,1),padding=(1,1),bias=False),\n#             torch.nn.BatchNorm2d(num_features=out_channels)\n        )\n    def forward(self,x):\n        x_resnet = self.conv(x)\n        out = x + x_resnet\n        return out\n\ndef phaseShift(inputs,scale,shape_1,shape_2):\n    x = torch.reshape(inputs,shape_1)\n    x = torch.reshape(x,[0,1,3,2,4])\n    return torch.reshape(x,shape_2)\n\n#使用PixelShuffle进行上采样\ndef PixelShuffle(inputs,scale = 2):\n    \"\"\"\n    :param inputs: 进行上采样的输入\n    :param scale: 上采样的倍率\n    :return:\n    \"\"\"\n    size = np.shape(inputs)\n    batch_size = size[0]\n    h = size[1]\n    w = size[2]\n    c = size[-1]\n    #进行上采样之后需要进行通道数1/4\n    channel_target = c // (scale * scale)\n    #获得上采样因子\n    channel_factor = c // channel_target\n    shape_1 = [batch_size,h,w,channel_factor // scale,channel_target//scale]\n    shape_2 = [batch_size,h * scale,w * scale]\n    #reshape and transpose for periods shuffle for each channel\n    input_split = torch.split(inputs,channel_target,dim=3)\n    output = torch.cat([phaseShift(x,scale,shape_1,shape_2) for x in input_split],dim=3)\n    return output\n\n# 生成器\nclass Generator(torch.nn.Module):\n    def __init__(self,upscale = 2,data_format = \"channels_last\"):\n        super(Generator, self).__init__()\n        self.upscale = 2\n        if data_format == \"channels_first\":\n            self._input_shape = [-1,3,32,32]\n            self.bn_axis = 1\n        else:\n            assert data_format == \"channels_last\"\n            self._input_shape=[-1,32,32,3]\n            self.bn_axis = 3\n\n        self.initial_conv = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(9, 9), stride=(1, 1),\n                            padding=(4,4),bias=False),\n            torch.nn.PReLU(num_parameters=1)\n        )\n\n        #使用残差模块\n        self.identityBlocks = [_IdentityBlock(in_channels=64,out_channels=64) for _ in range(16)]\n        self.Blocks = torch.nn.Sequential(\n            *self.identityBlocks\n        )\n\n        self.conv2 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1),\n                        padding=(1, 1))\n\n        # 由于进行了一次上采样，通道数减为原来的1/4，所以输入通道数为256=>64\n        self.upconv1 = torch.nn.Conv2d(in_channels=64,out_channels=256,kernel_size=(3,3),stride=(1,1),\n                        padding=(1,1))\n        self.prelu1 = torch.nn.PReLU(num_parameters=1)\n        #由于进行了一次上采样，通道数减为原来的1/4，所以输入通道数为256=>64\n        self.upconv2 = torch.nn.Conv2d(in_channels=64, out_channels=256, kernel_size=(3, 3), stride=(1, 1),\n                        padding=(1, 1))\n        self.prelu2 = torch.nn.PReLU(num_parameters=1)\n        self.conv3 = torch.nn.Conv2d(in_channels=64, out_channels=3, kernel_size=(9, 9), stride=(1, 1),\n                        padding=(4, 4), bias=False)\n    def forward(self,x):\n        # x = torch.reshape(x,self._input_shape)\n        x = self.initial_conv(x)\n\n        x_resnet = self.Blocks(x)\n        x_resnet = self.conv2(x_resnet)\n        x = x + x_resnet\n\n        #进行第一次上采样\n        x = self.upconv1(x)\n        x = torch.nn.PixelShuffle(self.upscale)(x)\n        x = self.prelu1(x)\n\n        #进行第二次上采样\n        x = self.upconv2(x)\n        x = torch.nn.PixelShuffle(self.upscale)(x)\n        x = self.prelu2(x)\n\n        x = self.conv3(x)\n        x = torch.tanh(x)\n\n        return x\n\n\nif __name__ == '__main__':\n    x = torch.randn(size = (1,3,32,32))\n    gen = Generator(upscale=2,data_format=\"channels_first\")\n    summary(gen,input_size=(1,3,32,32))\n    print(gen(x).shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T07:38:29.374322Z","iopub.execute_input":"2023-05-17T07:38:29.374792Z","iopub.status.idle":"2023-05-17T07:38:29.650675Z","shell.execute_reply.started":"2023-05-17T07:38:29.374752Z","shell.execute_reply":"2023-05-17T07:38:29.649192Z"},"editable":false,"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"torch.Size([1, 3, 128, 128])\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n@Author : Keep_Trying_Go\n@Major  : Computer Science and Technology\n@Hobby  : Computer Vision\n@Time   : 2023/5/16 15:26\n\"\"\"\nimport os\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#保存模型\ndef save_model(model,optimizer,epoch):\n    \"\"\"\n    :param model:\n    :param epoch:\n    :return:\n    \"\"\"\n    print(\"=> Saving checkpoint\")\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    torch.save(checkpoint, os.path.join(str(epoch)+'gen.tar'))\n\n\ndef load_checkpoin(checkpoint_file, model, optimizer, lr):\n    print(\"=> Loading checkpoint\")\n    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n    for param_group in optimizer.param_group:\n        param_group[\"lr\"] = lr\n\ndef generate_and_save_images(save_dir,features,gen,epoch):\n    \"\"\"\n    :param save_dir:\n    :param gen:\n    :param epoch:\n    :return:\n    \"\"\"\n    predictions = gen(features)\n    pass\n\ndef draw(gen_loss,disc_loss):\n    \"\"\"\n    :param gen_loss:\n    :param disc_loss:\n    :return:\n    \"\"\"\n    plt.plot(range(1, len(gen_loss) + 1), gen_loss, label='genLoss')\n    plt.plot(range(1, len(disc_loss) + 1), disc_loss, label='discLoss')\n    plt.legend()\n    plt.title('GEN_DISC-LOSS')\n    plt.savefig('logs/figure.png')\n\ndef save_images(model,epoch,step,val_loader):\n    imgs = model(val_loader).detach().cpu().numpy()\n    fig = plt.figure(figsize=(2,2))\n    imgs = np.squeeze(imgs)\n    for i in range(4):\n        plt.subplot(2,2,i + 1)\n        plt.imshow((imgs[i] + 1) / 2)\n        plt.axis(\"off\")\n    plt.savefig(os.path.join(config.SAVE_IMAGES,str(epoch)+\"_\"+str(step)+'.png'))","metadata":{"execution":{"iopub.status.busy":"2023-05-17T07:38:37.470958Z","iopub.execute_input":"2023-05-17T07:38:37.471360Z","iopub.status.idle":"2023-05-17T07:38:37.485752Z","shell.execute_reply.started":"2023-05-17T07:38:37.471326Z","shell.execute_reply":"2023-05-17T07:38:37.484591Z"},"editable":false,"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T07:23:40.859271Z","iopub.execute_input":"2023-05-17T07:23:40.859978Z","iopub.status.idle":"2023-05-17T07:23:41.085477Z","shell.execute_reply.started":"2023-05-17T07:23:40.859943Z","shell.execute_reply":"2023-05-17T07:23:41.084479Z"},"editable":false,"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{"editable":false}},{"cell_type":"code","source":"\"\"\"\n@Author : Keep_Trying_Go\n@Major  : Computer Science and Technology\n@Hobby  : Computer Vision\n@Time   : 2023/5/16 16:23\n\"\"\"\n\nimport os\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom torchinfo import summary\nimport torchvision.models.vgg\nfrom torch.utils.data import DataLoader,Dataset\n\n\ndef vgg19():\n    \"\"\"\n    vgg19主要是用来在这里提取特征，进生成器生成的图片输入到vgg19中和将标签图片输入到vgg19中，\n    将两个样本最后输出的特征之间的距离作为最后计算损失值\n    :return:\n    \"\"\"\n    vgg19 = torchvision.models.vgg.vgg19(pretrained = True,progress = True)\n    # vgg19.classifier = torch.nn.Sequential()\n    # vgg19.avgpool = torch.nn.Sequential()\n    #去掉最后的全连接层和avg_pooling层\n    vgg19 = torch.nn.Sequential(*(list(vgg19.children())[:-2])[0][:36])\n    # summary(vgg19,input_size=(1,3,128,128))\n    # print(vgg19)\n    return vgg19\n\n#生成器损失\ndef create_g_loss(d_output,g_output,labels,loss_model,loss_fn_gen):\n    \"\"\"\n    :param d_output: 判别器输出结果\n    :param g_output: 生成器生成的图片\n    :param labels: 标签值\n    :param loss_model: 损失模型\n    :return:\n    \"\"\"\n    gene_ce_loss = loss_fn_gen(torch.ones_like(d_output),d_output)\n    # print('labels.shape: {}'.format(np.shape(labels)))\n    # print('g_output.shape: {}'.format(np.shape(g_output)))\n    vgg_loss = torch.mean(torch.square(loss_model(labels) - loss_model(g_output)))\n    g_loss = vgg_loss + LOSS_RATIO * gene_ce_loss\n    return g_loss\n\n#判别器损失值\ndef create_d_loss(disc_real_output,disc_fake_output,loss_fn_real,loss_fn_fake):\n    \"\"\"\n    :param disc_real_output: 标签值输入到判别器的输出结果\n    :param disc_fake_output: 生成器生成的图片输入到判别器中的输出结果\n    :return:\n    \"\"\"\n    disc_real_loss = loss_fn_real(torch.ones_like(disc_real_output),disc_real_output)\n    disc_fake_loss = loss_fn_fake(torch.zeros_like(disc_fake_output),disc_fake_output)\n    disc_loss =  disc_fake_loss + disc_real_loss\n    return disc_loss\n\ndef train_step(features,labels,loss_model,gen,disc,opt_gen,opt_disc,loss_fn_gen,loss_fn_real,loss_fn_fake):\n    \"\"\"\n    :param features: 低分辨率图像\n    :param labels: 高分辨率图像\n    :param loss_model: 使用VGG19计算输入图像的特征\n    :param gen:生成器\n    :param disc:判别器\n    :param opt_gen:生成器优化器\n    :param opt_disc:判别器优化器\n    :return:\n    \"\"\"\n    fake_img = gen(features)\n    real_disc = disc(labels)\n    fake_disc = disc(fake_img)\n    g_loss = create_g_loss(fake_disc,fake_img,labels,loss_model,loss_fn_gen)\n    #注意fake_disc.detach()\n    d_loss = create_d_loss(real_disc,fake_disc.detach(),loss_fn_real,loss_fn_fake)\n\n    opt_gen.zero_grad()\n    g_loss.requires_grad_(True)\n    g_loss.backward(retain_graph=True)\n    opt_gen.step()\n\n    opt_disc.zero_grad()\n    d_loss.requires_grad_(True)\n    d_loss.backward(retain_graph=True)\n    opt_disc.step()\n    return g_loss,d_loss\n\n\n\ndef train():\n    #加载模型\n    gen = Generator().to(DEVICE)\n    disc = Discriminator(in_channels=3,out_channels=1).to(DEVICE)\n    #加载数据集\n    trainDataset = MyDataset(root_dir=\"/kaggle/input/celebadataset/celeba/train\")\n    valDataset = MyDataset(root_dir=\"/kaggle/input/celebadataset/celeba/val\")\n    trainLoader = DataLoader(\n        dataset=trainDataset,\n        batch_size=BATCH_SIZE_T,\n        shuffle=True,\n        num_workers=0\n    )\n    valLoader = DataLoader(\n        dataset=valDataset,\n        batch_size=BATCH_SIZE_V,\n        shuffle=True,\n        num_workers=0\n    )\n    #定义优化器\n    opt_gen = torch.optim.Adam(params=gen.parameters(),lr = LEARNING_RATIO,\n                               betas=(BEAT1,BEAT2),eps=EPSILON)\n    opt_disc = torch.optim.Adam(params=disc.parameters(),lr = LEARNING_RATIO,\n                                betas=(BEAT1,BEAT2),eps=EPSILON)\n\n    #定义损失函数\n    loss_fn_gen = torch.nn.BCELoss()\n    loss_fn_real = torch.nn.BCELoss()\n    loss_fn_fake = torch.nn.BCELoss()\n\n    loss_model = vgg19()\n    loss_model = loss_model.to(DEVICE)\n\n    gen_loss = []\n    disc_loss = []\n    for epoch in range(NUM_EPOCHS):\n        all_g_cost,all_d_cost = 0,0\n        loop = tqdm(trainLoader,leave=True)\n        loop.set_description(desc=\"training: \")\n        gen.train()\n        disc.train()\n        for step,data in enumerate(loop):\n            imgs,labels = data\n            imgs,labels = imgs.to(DEVICE),labels.to(DEVICE)\n\n            g_loss,d_loss = train_step(imgs,labels,loss_model,gen,disc,opt_gen,opt_disc,\n                                       loss_fn_gen,loss_fn_real,loss_fn_fake)\n\n            all_g_cost += g_loss\n            all_d_cost += d_loss\n            loop.set_description(desc=\"training: \")\n\n            if step % 50 == 0 and step > 0:\n                loop.set_postfix(epoch = epoch,g_loss = g_loss,d_loss = d_loss)\n                print(\"--------------------------------------g_loss: {:.6f}--------------------------------------\".format(g_loss))\n                print(\"--------------------------------------d_loss: {:.6f}--------------------------------------\".format(d_loss))\n        gen_loss.append(all_g_cost / len(trainLoader))\n        disc_loss.append(all_d_cost / len(trainLoader))\n\n        gen.eval()\n        disc.eval()\n        with torch.no_grad():\n            loop = tqdm(valLoader,leave=True)\n            loop.set_description(desc=\"valing: \")\n            for step,data in enumerate(loop):\n                imgs,labels = data\n                imgs,labels = imgs.to(DEVICE),labels.to(DEVICE)\n\n                if step % 100 == 0 and step > 0:\n                    utils.save_images(gen,epoch,step,imgs)\n        if epoch % 10 == 0:\n            save_model(gen, opt_gen, epoch)\n    draw(gen_loss,disc_loss)\n\n\nif __name__ == '__main__':\n    # vgg19()\n    train()\n\n# import tensorflow as tf\n#\n# vgg19 = tf.keras.applications.vgg19.VGG19(include_top = False,weights='imagenet',input_shape=(128,128,3))\n# vgg19.trainable = False\n# for l in vgg19.layers:\n#     l.trainable = False\n#\n# loss_models = tf.keras.Model(inputs = vgg19.input,outputs = vgg19.get_layer(\"block5_conv4\").output)\n# loss_models.trainable = False\n# loss_models.summary()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T07:38:44.178532Z","iopub.execute_input":"2023-05-17T07:38:44.178942Z","iopub.status.idle":"2023-05-17T07:40:06.416596Z","shell.execute_reply.started":"2023-05-17T07:38:44.178911Z","shell.execute_reply":"2023-05-17T07:40:06.414871Z"},"editable":false,"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"training: :   0%|          | 0/645 [01:19<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 168\u001b[0m\n\u001b[1;32m    163\u001b[0m     draw(gen_loss,disc_loss)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# vgg19()\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# import tensorflow as tf\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# vgg19 = tf.keras.applications.vgg19.VGG19(include_top = False,weights='imagenet',input_shape=(128,128,3))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# loss_models.trainable = False\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# loss_models.summary()\u001b[39;00m\n","Cell \u001b[0;32mIn[13], line 136\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m imgs,labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    134\u001b[0m imgs,labels \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(DEVICE),labels\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m--> 136\u001b[0m g_loss,d_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdisc\u001b[49m\u001b[43m,\u001b[49m\u001b[43mopt_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43mopt_disc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mloss_fn_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn_real\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn_fake\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m all_g_cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m g_loss\n\u001b[1;32m    140\u001b[0m all_d_cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m d_loss\n","Cell \u001b[0;32mIn[13], line 74\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(features, labels, loss_model, gen, disc, opt_gen, opt_disc, loss_fn_gen, loss_fn_real, loss_fn_fake)\u001b[0m\n\u001b[1;32m     72\u001b[0m real_disc \u001b[38;5;241m=\u001b[39m disc(labels)\n\u001b[1;32m     73\u001b[0m fake_disc \u001b[38;5;241m=\u001b[39m disc(fake_img)\n\u001b[0;32m---> 74\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_g_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_disc\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfake_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn_gen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m#注意fake_disc.detach()\u001b[39;00m\n\u001b[1;32m     76\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m create_d_loss(real_disc,fake_disc\u001b[38;5;241m.\u001b[39mdetach(),loss_fn_real,loss_fn_fake)\n","Cell \u001b[0;32mIn[13], line 44\u001b[0m, in \u001b[0;36mcreate_g_loss\u001b[0;34m(d_output, g_output, labels, loss_model, loss_fn_gen)\u001b[0m\n\u001b[1;32m     41\u001b[0m gene_ce_loss \u001b[38;5;241m=\u001b[39m loss_fn_gen(torch\u001b[38;5;241m.\u001b[39mones_like(d_output),d_output)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# print('labels.shape: {}'.format(np.shape(labels)))\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# print('g_output.shape: {}'.format(np.shape(g_output)))\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m vgg_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39msquare(\u001b[43mloss_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg_output\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     45\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m vgg_loss \u001b[38;5;241m+\u001b[39m LOSS_RATIO \u001b[38;5;241m*\u001b[39m gene_ce_loss\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m g_loss\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (32) at non-singleton dimension 3"],"ename":"RuntimeError","evalue":"The size of tensor a (2) must match the size of tensor b (32) at non-singleton dimension 3","output_type":"error"}]},{"cell_type":"code","source":"import torch, gc\n\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T07:25:03.579115Z","iopub.status.idle":"2023-05-17T07:25:03.579629Z","shell.execute_reply.started":"2023-05-17T07:25:03.579360Z","shell.execute_reply":"2023-05-17T07:25:03.579385Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]}]}